/*
MongoDB Atlas Administration API

The MongoDB Atlas Administration API allows developers to manage all components in MongoDB Atlas. To learn more, review the [Administration API overview](https://www.mongodb.com/docs/atlas/api/atlas-admin-api/). This OpenAPI specification covers all of the collections with the exception of Alerts, Alert Configurations, and Events. Refer to the [legacy documentation](https://www.mongodb.com/docs/atlas/reference/api-resources/) for the specifications of these resources.

API version: 2.0
*/

// Code generated by OpenAPI Generator (https://openapi-generator.tech); DO NOT EDIT.

package v1alpha

import (
	"encoding/json"
	"fmt"
)

// FTSAnalyzersViewTokenizer - Tokenizer that you want to use to create tokens. Tokens determine how Atlas Search splits up text into discrete chunks for indexing.
type FTSAnalyzersViewTokenizer struct {
	TokenizeredgeGram *TokenizeredgeGram
	Tokenizerkeyword *Tokenizerkeyword
	TokenizernGram *TokenizernGram
	TokenizerregexCaptureGroup *TokenizerregexCaptureGroup
	TokenizerregexSplit *TokenizerregexSplit
	Tokenizerstandard *Tokenizerstandard
	TokenizeruaxUrlEmail *TokenizeruaxUrlEmail
	Tokenizerwhitespace *Tokenizerwhitespace
}

// TokenizeredgeGramAsFTSAnalyzersViewTokenizer is a convenience function that returns TokenizeredgeGram wrapped in FTSAnalyzersViewTokenizer
func TokenizeredgeGramAsFTSAnalyzersViewTokenizer(v *TokenizeredgeGram) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		TokenizeredgeGram: v,
	}
}

// TokenizerkeywordAsFTSAnalyzersViewTokenizer is a convenience function that returns Tokenizerkeyword wrapped in FTSAnalyzersViewTokenizer
func TokenizerkeywordAsFTSAnalyzersViewTokenizer(v *Tokenizerkeyword) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		Tokenizerkeyword: v,
	}
}

// TokenizernGramAsFTSAnalyzersViewTokenizer is a convenience function that returns TokenizernGram wrapped in FTSAnalyzersViewTokenizer
func TokenizernGramAsFTSAnalyzersViewTokenizer(v *TokenizernGram) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		TokenizernGram: v,
	}
}

// TokenizerregexCaptureGroupAsFTSAnalyzersViewTokenizer is a convenience function that returns TokenizerregexCaptureGroup wrapped in FTSAnalyzersViewTokenizer
func TokenizerregexCaptureGroupAsFTSAnalyzersViewTokenizer(v *TokenizerregexCaptureGroup) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		TokenizerregexCaptureGroup: v,
	}
}

// TokenizerregexSplitAsFTSAnalyzersViewTokenizer is a convenience function that returns TokenizerregexSplit wrapped in FTSAnalyzersViewTokenizer
func TokenizerregexSplitAsFTSAnalyzersViewTokenizer(v *TokenizerregexSplit) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		TokenizerregexSplit: v,
	}
}

// TokenizerstandardAsFTSAnalyzersViewTokenizer is a convenience function that returns Tokenizerstandard wrapped in FTSAnalyzersViewTokenizer
func TokenizerstandardAsFTSAnalyzersViewTokenizer(v *Tokenizerstandard) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		Tokenizerstandard: v,
	}
}

// TokenizeruaxUrlEmailAsFTSAnalyzersViewTokenizer is a convenience function that returns TokenizeruaxUrlEmail wrapped in FTSAnalyzersViewTokenizer
func TokenizeruaxUrlEmailAsFTSAnalyzersViewTokenizer(v *TokenizeruaxUrlEmail) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		TokenizeruaxUrlEmail: v,
	}
}

// TokenizerwhitespaceAsFTSAnalyzersViewTokenizer is a convenience function that returns Tokenizerwhitespace wrapped in FTSAnalyzersViewTokenizer
func TokenizerwhitespaceAsFTSAnalyzersViewTokenizer(v *Tokenizerwhitespace) FTSAnalyzersViewTokenizer {
	return FTSAnalyzersViewTokenizer{
		Tokenizerwhitespace: v,
	}
}


// Unmarshal JSON data into one of the pointers in the struct
func (dst *FTSAnalyzersViewTokenizer) UnmarshalJSON(data []byte) error {
	var err error
	match := 0
	// try to unmarshal data into TokenizeredgeGram
	err = json.Unmarshal(data, &dst.TokenizeredgeGram)
	if err == nil {
		jsonTokenizeredgeGram, _ := json.Marshal(dst.TokenizeredgeGram)
		if string(jsonTokenizeredgeGram) == "{}" { // empty struct
			dst.TokenizeredgeGram = nil
		} else {
			match++
		}
	} else {
		dst.TokenizeredgeGram = nil
	}

	// try to unmarshal data into Tokenizerkeyword
	err = json.Unmarshal(data, &dst.Tokenizerkeyword)
	if err == nil {
		jsonTokenizerkeyword, _ := json.Marshal(dst.Tokenizerkeyword)
		if string(jsonTokenizerkeyword) == "{}" { // empty struct
			dst.Tokenizerkeyword = nil
		} else {
			match++
		}
	} else {
		dst.Tokenizerkeyword = nil
	}

	// try to unmarshal data into TokenizernGram
	err = json.Unmarshal(data, &dst.TokenizernGram)
	if err == nil {
		jsonTokenizernGram, _ := json.Marshal(dst.TokenizernGram)
		if string(jsonTokenizernGram) == "{}" { // empty struct
			dst.TokenizernGram = nil
		} else {
			match++
		}
	} else {
		dst.TokenizernGram = nil
	}

	// try to unmarshal data into TokenizerregexCaptureGroup
	err = json.Unmarshal(data, &dst.TokenizerregexCaptureGroup)
	if err == nil {
		jsonTokenizerregexCaptureGroup, _ := json.Marshal(dst.TokenizerregexCaptureGroup)
		if string(jsonTokenizerregexCaptureGroup) == "{}" { // empty struct
			dst.TokenizerregexCaptureGroup = nil
		} else {
			match++
		}
	} else {
		dst.TokenizerregexCaptureGroup = nil
	}

	// try to unmarshal data into TokenizerregexSplit
	err = json.Unmarshal(data, &dst.TokenizerregexSplit)
	if err == nil {
		jsonTokenizerregexSplit, _ := json.Marshal(dst.TokenizerregexSplit)
		if string(jsonTokenizerregexSplit) == "{}" { // empty struct
			dst.TokenizerregexSplit = nil
		} else {
			match++
		}
	} else {
		dst.TokenizerregexSplit = nil
	}

	// try to unmarshal data into Tokenizerstandard
	err = json.Unmarshal(data, &dst.Tokenizerstandard)
	if err == nil {
		jsonTokenizerstandard, _ := json.Marshal(dst.Tokenizerstandard)
		if string(jsonTokenizerstandard) == "{}" { // empty struct
			dst.Tokenizerstandard = nil
		} else {
			match++
		}
	} else {
		dst.Tokenizerstandard = nil
	}

	// try to unmarshal data into TokenizeruaxUrlEmail
	err = json.Unmarshal(data, &dst.TokenizeruaxUrlEmail)
	if err == nil {
		jsonTokenizeruaxUrlEmail, _ := json.Marshal(dst.TokenizeruaxUrlEmail)
		if string(jsonTokenizeruaxUrlEmail) == "{}" { // empty struct
			dst.TokenizeruaxUrlEmail = nil
		} else {
			match++
		}
	} else {
		dst.TokenizeruaxUrlEmail = nil
	}

	// try to unmarshal data into Tokenizerwhitespace
	err = json.Unmarshal(data, &dst.Tokenizerwhitespace)
	if err == nil {
		jsonTokenizerwhitespace, _ := json.Marshal(dst.Tokenizerwhitespace)
		if string(jsonTokenizerwhitespace) == "{}" { // empty struct
			dst.Tokenizerwhitespace = nil
		} else {
			match++
		}
	} else {
		dst.Tokenizerwhitespace = nil
	}

	if match > 1 { // more than 1 match
		// reset to nil
		dst.TokenizeredgeGram = nil
		dst.Tokenizerkeyword = nil
		dst.TokenizernGram = nil
		dst.TokenizerregexCaptureGroup = nil
		dst.TokenizerregexSplit = nil
		dst.Tokenizerstandard = nil
		dst.TokenizeruaxUrlEmail = nil
		dst.Tokenizerwhitespace = nil

		return fmt.Errorf("data matches more than one schema in oneOf(FTSAnalyzersViewTokenizer)")
	} else if match == 1 {
		return nil // exactly one match
	} else { // no match
		return fmt.Errorf("data failed to match schemas in oneOf(FTSAnalyzersViewTokenizer)")
	}
}

// Marshal data from the first non-nil pointers in the struct to JSON
func (src FTSAnalyzersViewTokenizer) MarshalJSON() ([]byte, error) {
	if src.TokenizeredgeGram != nil {
		return json.Marshal(&src.TokenizeredgeGram)
	}

	if src.Tokenizerkeyword != nil {
		return json.Marshal(&src.Tokenizerkeyword)
	}

	if src.TokenizernGram != nil {
		return json.Marshal(&src.TokenizernGram)
	}

	if src.TokenizerregexCaptureGroup != nil {
		return json.Marshal(&src.TokenizerregexCaptureGroup)
	}

	if src.TokenizerregexSplit != nil {
		return json.Marshal(&src.TokenizerregexSplit)
	}

	if src.Tokenizerstandard != nil {
		return json.Marshal(&src.Tokenizerstandard)
	}

	if src.TokenizeruaxUrlEmail != nil {
		return json.Marshal(&src.TokenizeruaxUrlEmail)
	}

	if src.Tokenizerwhitespace != nil {
		return json.Marshal(&src.Tokenizerwhitespace)
	}

	return nil, nil // no data in oneOf schemas
}

// Get the actual instance
func (obj *FTSAnalyzersViewTokenizer) GetActualInstance() (interface{}) {
	if obj == nil {
		return nil
	}
	if obj.TokenizeredgeGram != nil {
		return obj.TokenizeredgeGram
	}

	if obj.Tokenizerkeyword != nil {
		return obj.Tokenizerkeyword
	}

	if obj.TokenizernGram != nil {
		return obj.TokenizernGram
	}

	if obj.TokenizerregexCaptureGroup != nil {
		return obj.TokenizerregexCaptureGroup
	}

	if obj.TokenizerregexSplit != nil {
		return obj.TokenizerregexSplit
	}

	if obj.Tokenizerstandard != nil {
		return obj.Tokenizerstandard
	}

	if obj.TokenizeruaxUrlEmail != nil {
		return obj.TokenizeruaxUrlEmail
	}

	if obj.Tokenizerwhitespace != nil {
		return obj.Tokenizerwhitespace
	}

	// all schemas are nil
	return nil
}

type NullableFTSAnalyzersViewTokenizer struct {
	value *FTSAnalyzersViewTokenizer
	isSet bool
}

func (v NullableFTSAnalyzersViewTokenizer) Get() *FTSAnalyzersViewTokenizer {
	return v.value
}

func (v *NullableFTSAnalyzersViewTokenizer) Set(val *FTSAnalyzersViewTokenizer) {
	v.value = val
	v.isSet = true
}

func (v NullableFTSAnalyzersViewTokenizer) IsSet() bool {
	return v.isSet
}

func (v *NullableFTSAnalyzersViewTokenizer) Unset() {
	v.value = nil
	v.isSet = false
}

func NewNullableFTSAnalyzersViewTokenizer(val *FTSAnalyzersViewTokenizer) *NullableFTSAnalyzersViewTokenizer {
	return &NullableFTSAnalyzersViewTokenizer{value: val, isSet: true}
}

func (v NullableFTSAnalyzersViewTokenizer) MarshalJSON() ([]byte, error) {
	return json.Marshal(v.value)
}

func (v *NullableFTSAnalyzersViewTokenizer) UnmarshalJSON(src []byte) error {
	v.isSet = true
	return json.Unmarshal(src, &v.value)
}


